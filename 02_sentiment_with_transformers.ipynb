{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/transformers-with-python/blob/main/02_sentiment_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJYo3Fwbfw93"
      },
      "source": [
        "# Sentiment with Transformers\n",
        "\n",
        "The HuggingFace Transformers library is presently the most advanced and accessible library for building and using transformer models. As such, it will be what we primarily use throughout these notebooks.\n",
        "\n",
        "To apply sentiment analysis using the transformers library, we first need to decide on a model to use - as we will be applying a pretrained model, rather than starting from scratch. The list of models available can be found at [huggingface.co/models](https://www.huggingface.co/models).\n",
        "\n",
        "![Filter for Text Classification on HuggingFace models page](assets/hf_models_text_classification_filter.jpg)\n",
        "\n",
        "From the model page we select the **Text Classification** filter on the left of the page to filter for models that we can apply for text classification tasks immediately. We will be performing sentiment analysis on posts from */r/investing* (in this section we will be using the example given in `txt` below), which are finance oriented, so we can use the [finBERT](https://arxiv.org/abs/1908.10063) model [`ProsusAI/finbert`](https://huggingface.co/ProsusAI/finbert) which has been trained on financial articles for financial sentiment classification.\n",
        "\n",
        "FinBERT is ofcourse a BERT model, so when loading the model and tokenizer we will using BERT classes, and because we are performing *sequence classification* we will be using `BertForSequenceClassification`. Let's initialize our model and tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "bb1ae2bcc1be4292b998960a333ec51f",
            "216d61e5108644c28343ab7a201418ca",
            "8cd1329c81634f1aaeef42cbab89bd31",
            "6a4094bbe9cf4d9daa2f621bb2b5cd6d",
            "2b36702a2faa4ea39364870bab6e14dc"
          ]
        },
        "id": "GxuXAnz-fw96",
        "outputId": "b2e8ce28-210b-45ba-b560-8b7263410001"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb1ae2bcc1be4292b998960a333ec51f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "216d61e5108644c28343ab7a201418ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=112.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cd1329c81634f1aaeef42cbab89bd31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=252.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a4094bbe9cf4d9daa2f621bb2b5cd6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=758.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b36702a2faa4ea39364870bab6e14dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=437992753.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# initialize the tokenizer for BERT models\n",
        "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "# initialize the model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis with BERT\n",
        "This code is about importing and initializing a pre-trained BERT model and tokenizer for sentiment analysis, specifically using the FinBERT model which is tailored for financial text.\n",
        "\n",
        "---------------------------\n",
        "\n",
        "#Import necessary modules:\n",
        "\n",
        "    from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "##`transformers`:\n",
        "This is the Hugging Face Transformers library, offering tools to work with pre-trained models.\n",
        "\n",
        "##`BertForSequenceClassification`:\n",
        "\n",
        "This specific class is used when you want to use BERT for tasks like sentiment classification, where you need to assign a label or category to a piece of text.\n",
        "\n",
        "##`BertTokenizer`:\n",
        "This class is used to convert raw text into the numerical input format that BERT understands.\n",
        "\n",
        "-------------------\n",
        "This process is called tokenization.\n",
        "\n",
        "#Initialize the Tokenizer:\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "\n",
        "##`tokenizer`:\n",
        " A variable to store the tokenizer object.\n",
        "\n",
        "##`from_pretrained('ProsusAI/finbert')`:\n",
        "This loads a pre-trained tokenizer specific to the FinBERT model.\n",
        "\n",
        "This tokenizer is trained on financial text and knows how to break it down into tokens.\n",
        "\n",
        "This will download the necessary files if you don't have them already.\n",
        "\n",
        "----------------------\n",
        "#Initialize the Model:\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
        "\n",
        "\n",
        "##`model`:\n",
        "A variable to store the BERT model object.\n",
        "\n",
        "##`from_pretrained('ProsusAI/finbert')`:\n",
        " This line loads the pre-trained FinBERT model.\n",
        "\n",
        " FinBERT is a BERT model that's fine-tuned on financial data, making it better suited for financial sentiment analysis.\n",
        "\n",
        " Like the tokenizer, this will download the model weights if they are not already present.\n",
        "\n",
        "In essence, this code does the following:\n",
        "\n",
        "--------------------\n",
        "#Gets the tools (classes):\n",
        "\n",
        "Imports BertForSequenceClassification to do the sentiment classification and BertTokenizer to prepare the text.\n",
        "\n",
        "##Sets up the tokenizer:\n",
        "\n",
        "Creates a tokenizer object using the FinBERT tokenizer to break down text.\n",
        "\n",
        "##Sets up the model:\n",
        "\n",
        "Creates a model object using the pre-trained FinBERT model, ready for sentiment analysis on financial text."
      ],
      "metadata": {
        "id": "5FWMWV0Fmkn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F9E6MaHkmJgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cq_MgeFtl680"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9ECOH0Hfw98"
      },
      "source": [
        "The first time that this is run when using the `ProsusAI/finbert` model, it will be downloaded from the HuggingFace model repositories. We will be following a very similar process to that which we worked through for our Flair sentiment classifier, with some added steps to convert model output activations to class predictions.\n",
        "\n",
        "1. We tokenize our input text.\n",
        "\n",
        "2. Tokenized inputs are fed into the model, which outputs final layer **activations** (note *activations* are not *probabilities*).\n",
        "\n",
        "3. Convert those activations into probabilities using a softmax function (sigmoid for multiple classes).\n",
        "\n",
        "4. Take the **argmax** of those probabilities.\n",
        "\n",
        "5. *(Optional) Extract the probability of the winning class.*\n",
        "\n",
        "For step one, we will use the `encode_plus` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMJVBriSfw98",
        "outputId": "ce837b72-5138-48a0-be32-9612558b3bdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2445,  1996,  3522,  2091, 22299,  1999, 15768,  2926,  1999,\n",
              "          6627,  2029,  2003,  3497,  2000, 29486,  2004, 16189,  2562,  2183,\n",
              "          2039,  1010,  1045,  2245,  2009,  2052,  2022, 10975, 12672,  3372,\n",
              "          2000,  3745,  1996, 10831,  1997, 19920,  1999, 15745,  3802, 10343,\n",
              "          1010,  2517,  2039,  2200, 19957,  2011,  1031,  1996,  4562,  5430,\n",
              "          1033,  1006, 16770,  1024,  1013,  1013,  1996,  4783,  2906, 27454,\n",
              "          1012,  4942,  9153,  3600,  1012,  4012,  1013,  1052,  1013,  2569,\n",
              "          1011,  3179,  1011,  2097,  1011, 15745,  1011, 15697,  1011,  6271,\n",
              "          1007,  1012,  1996, 10831,  3310,  3952,  2013, 15745,  1005,  1055,\n",
              "          5665, 18515, 21272,  1998,  2200,  2312,  9583,  1999,  2235,  6178,\n",
              "          3316,  1012, 15745,  2003,  3140,  2000,  5271,  2049,  9583,  7188,\n",
              "          2049,  6381,  3802,  2546,  4152,  2718,  2007,  2041, 12314,  2015,\n",
              "          2004,  2003,  2926,  1996,  2553,  1999,  3006,  2091, 22299,  2015,\n",
              "          1012,  2023,  2071,  2486,  2200,  9145, 28763,  2015,  2012,  4895,\n",
              "          7011, 14550,  3085,  7597,  1998,  1996, 13831,  5823,  3632,  2046,\n",
              "          1037,  3893, 12247,  7077,  2877,  2046,  1037,  2331, 12313,  4372,\n",
              "          4588,  2075,  2130,  2062,  2041, 12314,  2015,  1998, 21659,  9132,\n",
              "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# this is our example text\n",
        "txt = (\"Given the recent downturn in stocks especially in tech which is likely to persist as yields keep going up, \"\n",
        "       \"I thought it would be prudent to share the risks of investing in ARK ETFs, written up very nicely by \"\n",
        "       \"[The Bear Cave](https://thebearcave.substack.com/p/special-edition-will-ark-invest-blow). The risks comes \"\n",
        "       \"primarily from ARK's illiquid and very large holdings in small cap companies. ARK is forced to sell its \"\n",
        "       \"holdings whenever its liquid ETF gets hit with outflows as is especially the case in market downturns. \"\n",
        "       \"This could force very painful liquidations at unfavorable prices and the ensuing crash goes into a \"\n",
        "       \"positive feedback loop leading into a death spiral enticing even more outflows and predatory shorts.\")\n",
        "\n",
        "tokens = tokenizer.encode_plus(txt, max_length=512, truncation=True, padding='max_length',\n",
        "                               add_special_tokens=True, return_tensors='pt')\n",
        "\n",
        "entokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explaing the Tokens\n",
        "\n",
        "---------------------------------\n",
        "##Token IDs:\n",
        "BERT uses a vocabulary where each word or subword is assigned a unique numerical ID.\n",
        "\n",
        "These IDs are what the numbers in the tensor represent.\n",
        "\n",
        "----------------------------------------\n",
        "##Special Tokens:\n",
        "\n",
        "Certain token IDs have specific meanings:\n",
        "\n",
        "##101:\n",
        "Represents the start of a sequence ([CLS] token)\n",
        "\n",
        "##102:\n",
        "Represents the end of a sequence or separation between sequences ([SEP] token)\n",
        "\n",
        "##0:\n",
        "Represents padding tokens ([PAD] tokens)\n",
        "\n",
        "##Padding:\n",
        "BERT models often require input sequences to be of a fixed length.\n",
        "\n",
        "If the original sequence is shorter, padding tokens (represented by 0s) are added to bring it to the required length.\n",
        "\n",
        "------------------------------------------\n",
        "#How it relates to your tensor:\n",
        "\n",
        "The tensor starts with 101, indicating the start of the sequence.\n",
        "\n",
        "It contains a series of numbers, each representing a word or subword in your input text.\n",
        "\n",
        "These numbers correspond to the token IDs in the BERT vocabulary.\n",
        "\n",
        "##You can see 102, which signifies the end of the actual sequence.\n",
        "\n",
        "The rest of the tensor is filled with 0s.\n",
        "\n",
        "This means the original text sequence was shorter than the required input length, so padding tokens were added to fill the gap.\n",
        "\n",
        "------------------------------------\n",
        "#Attention Mask\n",
        "The attention_mask tells the attention layers in BERT which words to calculate attention for.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "##Connection to input_ids:\n",
        "\n",
        "Each 1 value in the attention_mask directly corresponds to an actual token (a word or subword) in the input_ids tensor.\n",
        "\n",
        "##Ignoring Padding:\n",
        "\n",
        "Conversely, each 0 in the attention_mask corresponds to a padding token (0) in the input_ids tensor.\n",
        "\n",
        "##Attention Calculation:\n",
        "\n",
        "During the attention mechanism's calculations, the activations (values) associated with padding tokens are essentially multiplied by 0 due to the attention_mask.\n",
        "\n",
        "This effectively cancels them out, preventing the model from paying attention to these meaningless padding elements.\n",
        "\n",
        "In simpler terms:\n",
        "\n",
        "The attention_mask acts like a spotlight, highlighting the important parts of the input sequence (the actual words) and dimming the unimportant parts (the padding).\n",
        "\n",
        "It guides BERT's attention to focus only on the relevant tokens for understanding the meaning of the text.\n",
        "\n",
        "In your specific example:\n",
        "\n",
        "You've provided a portion of the attention_mask tensor, and it contains only 1s.\n",
        "\n",
        "This indicates that, up to that point in the sequence, all the tokens are real words or subwords and should be considered by the attention mechanism.\n",
        "\n",
        "If there were any padding tokens in the corresponding input_ids tensor, you would see 0s in the attention_mask at the same positions.\n",
        "\n",
        "----------------------------------\n",
        "#In simpler terms:\n",
        "\n",
        "Imagine you have a box that can hold 512 items (the required sequence length for BERT).\n",
        "\n",
        "You have a sentence with 10 words. You place those 10 words in the box, and to fill the remaining space (502 slots), you add empty placeholders (padding tokens represented by 0).\n",
        "\n",
        "The '101' and '102' are special markers to indicate the beginning and end of your actual sentence within the box."
      ],
      "metadata": {
        "id": "PLeYosD3qDd3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ9Ruuanfw99"
      },
      "source": [
        "Here we have specified a few arguments that require some explanation.\n",
        "\n",
        "* `max_length` - this tell the tokenizer the maximum number of tokens we want to see in each sample, for BERT we almost always use `512` as that is the length of sequences that BERT consumes.\n",
        "\n",
        "* `truncation` - if our input string `txt` contains more tokens than allowed (specified in `max_length` parameter) then we cut all tokens past the `max_length` limit.\n",
        "\n",
        "* `padding` - if our input string `txt` contains less tokens than specified by `max_length` then we pad the sequence with zeros (`0` is the token ID for *'[PAD]'* - BERTs padding token).\n",
        "\n",
        "* `add_special_tokens` - whether or not to add special tokens, when using BERT we always want this to be `True` unless we are adding them ourselves.\n",
        "\n",
        "| Token | ID | Description |\n",
        "| --- | --- | --- |\n",
        "| [PAD] | 0 | Used to fill empty space when input sequence is shorter than required sequence size for model |\n",
        "| [UNK] | 100 | If a word/character is not found in BERTs vocabulary it will be represented by this *unknown* token |\n",
        "| [CLS] | 101 | Represents the start of a sequence |\n",
        "| [SEP] | 102 | Seperator token to denote the end of a sequence and as a seperator where there are multiple sequences |\n",
        "| [MASK] | 103 | Token used for masking other tokens, used for masked language modeling |\n",
        "\n",
        "*Note that our tokenized sequence begins with `101`, the seperator token `102` can be found seperating the input sequence and padding tokens `0`.*\n",
        "\n",
        "* `return_tensors` - here we specify either `'pt'` to return PyTorch tensors, or `'tf'` to return TensorFlow tensors.\n",
        "\n",
        "The output produced includes **three** tensors in a dictionary format, `'input_ids'`, `'token_type_ids'`, and `'attention_mask'`. We can ignore `'token_type_ids'` as they are not used by BERT, the other two tensors are however.\n",
        "\n",
        "* `'input_ids'` are the token ID representations of our input text. These will be passed into an embedding array where vector representations of each word will be found and passed into the following BERT layers.\n",
        "\n",
        "* `'attention_mask'` tells the attention layers in BERT which words to calculate attention for. If you look at this tensors you will see that each `1` value maps to an input ID from the `'input_ids'` tensor, whereas each `0` value maps to a *padding token* from the `'attention_mask'` tensor. In the attention layer (activations mapping to padding tokens are multiplied by 0, and so are cancelled out).\n",
        "\n",
        "Now that we have our tokenized input, we can pass it onto our `model` for inference. We pass in our `tokens` as *\\*\\*kwargs* (key word arguments), which we can use thanks to our tokens being in a dictionary format. When a dictionary is passed as a \\*\\*kwargs argument, the keys will be taken literally as variable names and the respective values become the variable values. So these two approaches would do the same thing:\n",
        "\n",
        "```\n",
        "# without **kwargs\n",
        "random_func(var1='hello', var2='world')\n",
        "\n",
        "# with **kwargs\n",
        "input_dict = {'var1': 'hello', 'var2': 'world'}\n",
        "random_func(**input_dict)\n",
        "```\n",
        "\n",
        "Let's see how that looks for making predictions with our model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the use of `kwargs`**\n",
        "\n",
        "##**kwargs (keyword arguments)\n",
        "\n",
        " is a way to pass a variable number of keyword arguments to a function.\n",
        "\n",
        "It's essentially a dictionary where the keys become the argument names, and the values become the argument values.\n",
        "\n",
        "-------------------------\n",
        "How it Works in Your Example\n",
        "\n",
        "input_dict: You create a dictionary called input_dict with two key-value pairs: 'var1': 'hello' and 'var2': 'world'.\n",
        "\n",
        "`random_func(input_dict):** When you callrandom_funcwith**input_dict`, Python unpacks the dictionary.\n",
        "\n",
        "The key 'var1' is treated as an argument name, and its value 'hello' is assigned to that argument.\n",
        "\n",
        "Similarly, the key 'var2' becomes an argument name, and 'world' is assigned to it.\n",
        "\n",
        "Equivalent Call\n",
        "\n",
        "The code you provided is functionally equivalent to this:\n",
        "\n",
        "\n",
        "random_func(var1='hello', var2='world')\n",
        "\n",
        "----------------------\n",
        "\n",
        "#Benefits of Using `kwargs`**\n",
        "\n",
        "##Flexibility:\n",
        "You can easily pass different sets of arguments to the function without changing its definition.\n",
        "\n",
        "##Readability:\n",
        "It can make function calls more clear when you have many arguments.\n",
        "\n",
        "##Extensibility:\n",
        "You can add new arguments to the dictionary without modifying the function call.\n",
        "\n",
        "---------------------------\n",
        "#Important Considerations\n",
        "\n",
        "#Function Definition:\n",
        "\n",
        "The function you're calling (random_func in this case) needs to be designed to accept keyword arguments.\n",
        "\n",
        "#Argument Names:\n",
        "The keys in the dictionary must match the expected argument names in the function definition."
      ],
      "metadata": {
        "id": "tu6-DE4QxP0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGFb510sfw99",
        "outputId": "f76a375c-2db2-4199-dcef-370e5f5f4256"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8200,  2.4484,  0.0216]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = model(**tokens)\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiM71BHQfw99"
      },
      "source": [
        "You will notice here that the output logits tensor **cannot** be a set of probabilities because probability values must be within the range 0-1. These are infact the final output activations from BERT, to convert these into probabilities we must apply a **softmax** function. We will be using the PyTorch implementation of softmax for this, which we import from `torch.nn.functional`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULNfu9I3fw99",
        "outputId": "4e8e7be2-6905-42df-e639-65cc8c053d74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0127, 0.9072, 0.0801]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# apply softmax to the logits output tensor of our model (in index 0) across dimension -1\n",
        "probs = F.softmax(output[0], dim=-1)\n",
        "\n",
        "probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A32W-CQmfw99"
      },
      "source": [
        "*(We use `dim=-1` as **-1** signifies our tensors final dimension, so if we had a 3D tensor with dims `[0, 1, 2]` writing `dim=-1` is the equivalent to writing `dim=2`. In this case if we wrote `dim=-2` this would be the equivalent to writing `dim=1`. For a 2D tensor with dims `[0, 1]`, `dim=-1` is the equivalent of `dim=1`.)*\n",
        "\n",
        "Now we have a tensor containing three classes, all with outputs within the probability range of 0-1, these are our probabilities! We can see that class index **1** has the highest probability with a value of **0.9072**. We can use PyTorch's argmax function to extract this, we can use `argmax` after importing `torch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_vz539pfw9-",
        "outputId": "00ea6300-6631-4d66-ea0b-23e9f50fc26a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "pred = torch.argmax(probs)\n",
        "\n",
        "pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skZxzT_Lfw9-"
      },
      "source": [
        "Argmax outputs our winning class as **1** as expected. To convert this value from a PyTorch tensor to a Python integer we can use the `item` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RuBV1Wyfw9-",
        "outputId": "457d5c99-95eb-46fb-cc97-75b1fd236abd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred.item()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}