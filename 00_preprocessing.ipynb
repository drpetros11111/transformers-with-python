{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/transformers-with-python/blob/main/00_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgEq5DlodTdT"
      },
      "source": [
        "# Preprocessing Data\n",
        "\n",
        "We will be using the [Rotten Tomatoes movie reviews dataset](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou4JTUdddTdV",
        "outputId": "48a7ef02-8841-4847-a370-9c29ebc6ddd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading train.tsv.zip to .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 1.28M/1.28M [00:00<00:00, 6.49MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading test.tsv.zip to .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████| 494k/494k [00:00<00:00, 4.53MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "for file in ['train.tsv', 'test.tsv']:\n",
        "    api.competition_download_file('sentiment-analysis-on-movie-reviews', f'{file}.zip', path='./')\n",
        "\n",
        "    with zipfile.ZipFile(f'{file}.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('./')\n",
        "\n",
        "    os.remove(f'{file}.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17cX8uIddTdX"
      },
      "source": [
        "---\n",
        "\n",
        "## Preparing Data\n",
        "\n",
        "We will start by reading the data into a Pandas Dataframe using th `read_csv` function. Because we're working with *.tsv* (*tab seperate values*) files we need to specify that we will be taking tab characters as the delimiters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTbcitrndTdX",
        "outputId": "6d29ae46-05a2-463e-e7b6-b90923586bb3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  SentenceId                                             Phrase  \\\n",
              "0         1           1  A series of escapades demonstrating the adage ...   \n",
              "1         2           1  A series of escapades demonstrating the adage ...   \n",
              "2         3           1                                           A series   \n",
              "3         4           1                                                  A   \n",
              "4         5           1                                             series   \n",
              "\n",
              "   Sentiment  \n",
              "0          1  \n",
              "1          2  \n",
              "2          2  \n",
              "3          2  \n",
              "4          2  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('train.tsv', sep='\\t')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7fRkVyQdTdY"
      },
      "source": [
        "The *Phrase* column contains all of our text data that we will be processing. We can also see that there are many copies through *segments* of the same answer (note that the *SentenceId* value for each of these copies is identical). We can reduce the amount of noise in our dataset by removing these duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3D2QWdxdTdY",
        "outputId": "b44da4d9-c6e3-4470-ebd0-c129d84c4f0c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  SentenceId                                             Phrase  \\\n",
              "0         1           1  A series of escapades demonstrating the adage ...   \n",
              "1         2           1  A series of escapades demonstrating the adage ...   \n",
              "2         3           1                                           A series   \n",
              "3         4           1                                                  A   \n",
              "4         5           1                                             series   \n",
              "\n",
              "   Sentiment  \n",
              "0          1  \n",
              "1          2  \n",
              "2          2  \n",
              "3          2  \n",
              "4          2  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#df = df.drop_duplicates(subset=['SentenceId'], keep='first')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SilEO-FZdTdY"
      },
      "source": [
        "Let's check the distribution of sentiment classes across our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O_c9uVNdTdZ",
        "outputId": "6df604b0-3ade-49bc-b8f1-86df626bb686"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df['Sentiment'].value_counts().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Graph\n",
        "Certainly, let's break down the code df['Sentiment'].value_counts().plot(kind='bar') step-by-step within the context of your Colab notebook.\n",
        "\n",
        "----------------------------------------\n",
        "#Explanation\n",
        "\n",
        "This line of code is designed to visualize the distribution of sentiment classes in your dataset using a bar chart. Here's what each part does:\n",
        "\n",
        "    df['Sentiment']\n",
        "\n",
        "This part selects the 'Sentiment' column from your Pandas DataFrame (df).\n",
        "\n",
        "In your dataset, the 'Sentiment' column contains the sentiment labels for each movie review phrase, ranging from 0 to 4.\n",
        "\n",
        "    .value_counts():\n",
        "\n",
        "This is a Pandas Series method that calculates the frequency of each unique value within the 'Sentiment' column.\n",
        "\n",
        "It returns a new Series where:\n",
        "The index is the unique sentiment labels (0, 1, 2, 3, 4).\n",
        "\n",
        "The values are the counts of how many times each sentiment label appears in the column.\n",
        "    .plot(kind='bar'):\n",
        "\n",
        "This is another Pandas Series method that generates a plot based on the data in the Series.\n",
        "\n",
        "    kind='bar'\n",
        "\n",
        "specifies that you want to create a bar chart.\n",
        "\n",
        "Pandas will use the index of the Series (the sentiment labels) as the x-axis labels and the values (the counts) as the bar heights.\n",
        "\n",
        "--------------------------------------\n",
        "#In Summary\n",
        "\n",
        "The code effectively does the following:\n",
        "\n",
        "Looks at the 'Sentiment' column.\n",
        "\n",
        "Counts how many times each sentiment value occurs.\n",
        "\n",
        "Creates a bar chart that visually shows the distribution of those sentiment counts, with each bar representing a different sentiment label.\n",
        "Why is this useful?\n",
        "\n",
        "This bar chart is a valuable tool for:\n",
        "\n",
        "##Understanding class distribution:\n",
        "\n",
        "You can quickly see if the dataset is balanced (roughly equal numbers of each sentiment) or imbalanced (some sentiments are much more frequent than others).\n",
        "\n",
        "##Identifying potential issues:\n",
        "\n",
        "If one sentiment is extremely rare, it might be difficult for a model to learn to predict it accurately.\n",
        "\n",
        "Gaining initial insights: You get a quick visual sense of the sentiment makeup of your dataset.\n",
        "\n",
        "How to see the output\n",
        "\n",
        "To see the bar plot output, you need to execute the cell that contains this code in the Colab notebook. After you run it, the generated bar chart will be displayed below the code cell."
      ],
      "metadata": {
        "id": "nIB485Mver6L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzIS7k69dTdZ"
      },
      "source": [
        "We will be tokenizing this text to create two input tensors; our input IDs, and attention mask.\n",
        "\n",
        "We will contain our tensors within two numpy arrays, which will be of dimensions `len(df) * 512` - the `512` is the sequence length of our tokenized sequences for BERT, and `len(df)` the number of samples in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4VMz9s1dTda",
        "outputId": "2ac1fc3c-6ff9-4ebe-a79e-3ab7f373230c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(156060, 512)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "seq_len = 512\n",
        "num_samples = len(df)\n",
        "\n",
        "num_samples, seq_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEp6LvzWdTda"
      },
      "source": [
        "Now we can begin tokenizing with a `BertTokenizer`, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "be52ef3bd3664ed3b7e3f995db12b67d",
            "3142f4d4e66e43cabef59e24f38d8a07",
            "7251eb80099d4328982b9cafa261ecf2",
            "e87ba68d045c4d6486047aec8a7976ca"
          ]
        },
        "id": "gCskOV14dTda",
        "outputId": "0165c3ac-b5d8-4293-f850-097b6d9f33f0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be52ef3bd3664ed3b7e3f995db12b67d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3142f4d4e66e43cabef59e24f38d8a07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7251eb80099d4328982b9cafa261ecf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e87ba68d045c4d6486047aec8a7976ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# tokenize - this time returning Numpy tensors\n",
        "tokens = tokenizer(df['Phrase'].tolist(), max_length=seq_len, truncation=True,\n",
        "                   padding='max_length', add_special_tokens=True,\n",
        "                   return_tensors='np')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Explanation:\n",
        "\n",
        "----------------------\n",
        "#1. from transformers import BertTokenizer\n",
        "\n",
        "What it does: This line imports the BertTokenizer class from the transformers library (Hugging Face's library for working with transformer models).\n",
        "\n",
        "Why it's needed: The BertTokenizer is a specialized tokenizer designed to work with BERT (Bidirectional Encoder Representations from Transformers) models.\n",
        "\n",
        "It handles the specific text preprocessing steps that BERT requires.\n",
        "\n",
        "How to use it: Ensure that you have installed the transformers library in your Colab notebook. You can do this with: !pip install transformers==4.35.0\n",
        "\n",
        "-------------------\n",
        "#2. # initialize tokenizer\n",
        "\n",
        "This is just a comment.\n",
        "\n",
        "------------------\n",
        "#3. tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "What it does: This line initializes an instance of the BertTokenizer.\n",
        "\n",
        "from_pretrained('bert-base-cased'): This part is crucial. It tells the tokenizer to load a pre-trained vocabulary and configuration specifically for the bert-base-cased model.\n",
        "\n",
        "bert-base-cased: This is the name of a specific pre-trained BERT model.\n",
        "\n",
        "bert-base: Indicates it's the base-sized BERT model.\n",
        "\n",
        "cased: Means that the model was trained with case sensitivity (it distinguishes between \"Cat\" and \"cat\").\n",
        "\n",
        "When you use from_pretrained, the transformers library will automatically download the necessary files (vocabulary, etc.) for this model the first time you run this code.\n",
        "\n",
        "These files will then be cached for faster loading on subsequent runs.\n",
        "\n",
        "Why it's important: By using a pre-trained vocabulary, you ensure that your text is tokenized in the same way as the BERT model was trained.\n",
        "\n",
        "This is essential for the model to understand your input.\n",
        "\n",
        "---------------------\n",
        "#4. # tokenize - this time returning Numpy tensors\n",
        "\n",
        "This is just a comment.\n",
        "\n",
        "------------------------\n",
        "#5. tokens = tokenizer(df['Phrase'].tolist(), max_length=seq_len, truncation=True, padding='max_length', add_special_tokens=True, return_tensors='np')\n",
        "\n",
        "What it does: This is the core tokenization step. It takes your text data and converts it into a numerical format that BERT can process.\n",
        "\n",
        "tokenizer(...): You're calling the tokenizer object (that you initialized earlier) as if it were a function. This is the standard way to use tokenizers in the transformers library.\n",
        "\n",
        "df['Phrase'].tolist():\n",
        "\n",
        "df['Phrase']: Selects the 'Phrase' column from your DataFrame.\n",
        "\n",
        ".tolist(): Converts the Pandas Series into a regular Python list of strings. The tokenizer expects a list of texts as input.\n",
        "\n",
        "max_length=seq_len:\n",
        "Sets the maximum length of the tokenized sequence to 512 (as you defined seq_len earlier).\n",
        "\n",
        "Sequences longer than this will be truncated.\n",
        "\n",
        "truncation=True:\n",
        "Enables truncation. If a sequence exceeds max_length, it will be cut off.\n",
        "\n",
        "padding='max_length':\n",
        "Enables padding. If a sequence is shorter than max_length, it will be padded with a special padding token ([PAD]) until it reaches the max_length.\n",
        "\n",
        " This is important because BERT expects all input sequences to have the same length.\n",
        "add_special_tokens=True:\n",
        "Tells the tokenizer to add special tokens that BERT uses:\n",
        "[CLS] (classification token): Added at the beginning of each sequence.\n",
        "[SEP] (separator token): Added at the end of each sequence.\n",
        "return_tensors='np':\n",
        "This is crucial for your workflow. It specifies that the tokenizer should return the tokenized data as NumPy arrays (tensors) instead of the default PyTorch tensors.\n",
        "In your notebook you are already using Numpy, this simplifies things.\n",
        "Output:\n",
        "This function will return a dictionary called tokens that contains the tokenized sequences as Numpy arrays.\n",
        "In Summary\n",
        "\n",
        "This block of code takes the 'Phrase' column from your DataFrame, tokenizes it using the pre-trained bert-base-cased tokenizer, and returns the results as NumPy arrays (to be saved to a file). It also handles truncation and padding to ensure all tokenized sequences have the same length and are formatted correctly for BERT."
      ],
      "metadata": {
        "id": "eHxbzDaXgQkB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHtNUdD2dTda"
      },
      "source": [
        "Which returns us three numpy arrays - *input_ids*, *token_type_ids*, and *attention_mask*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk90HLwxdTda",
        "outputId": "67fded60-cb82-4a37-8d6c-804f36d789c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC9IwiTJdTdb",
        "outputId": "697819b2-a162-4d41-f164-cbae475ccdc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  101,   138,  1326, ...,     0,     0,     0],\n",
              "       [  101,   138,  1326, ...,     0,     0,     0],\n",
              "       [  101,   138,  1326, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [  101, 13936, 25265, ...,     0,     0,     0],\n",
              "       [  101, 13936, 25265, ...,     0,     0,     0],\n",
              "       [  101, 15107,  1103, ...,     0,     0,     0]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens['input_ids'][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mS5tzM3dTdb",
        "outputId": "2cb3fef6-82de-4af1-f533-0d6056351543"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens['attention_mask'][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PcMgRVpdTdb"
      },
      "source": [
        "And now we save them to file as Numpy binary files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArtZkxtzdTdb"
      },
      "outputs": [],
      "source": [
        "with open('movie-xids.npy', 'wb') as f:\n",
        "    np.save(f, tokens['input_ids'])\n",
        "with open('movie-xmask.npy', 'wb') as f:\n",
        "    np.save(f, tokens['attention_mask'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyOAXg8JdTdc"
      },
      "source": [
        "Now that we have them on file, we can delete the in-memory arrays to free up memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8EBzuxCdTdc"
      },
      "outputs": [],
      "source": [
        "del tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJrnzrDDdTdc"
      },
      "source": [
        "Our input tensors are prepared, but we haven't touched our target data yet. So, let's move onto that.\n",
        "\n",
        "Presently our target data is a set of integer values (representing sentiment classes) in the *Sentiment* column of our dataframe `df`. We need to extract these values and *one-hot* encode them into another numpy array, which will have the dimensions `len(df) * number of label classes`. Again, we will initialize a numpy zero array beforehand, but we won't populate it row by row - we will use some fancy indexing techniques instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ9i48zHdTdc"
      },
      "outputs": [],
      "source": [
        "# first extract sentiment column\n",
        "arr = df['Sentiment'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjyzhwQ9dTdc",
        "outputId": "6b89f043-36d8-4abc-fb69-fb4cb6673003"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(156060, 5)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we then initialize the zero array\n",
        "labels = np.zeros((num_samples, arr.max()+1))\n",
        "labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm4OHjh-dTdc"
      },
      "source": [
        "We are able to use `arr.max()+1` to define our second dimension here because we have the values *\\[0, 1, 2, 3, 4\\]* in our *Sentiment* column, there are **five** unique labels which means we need our labels array to have five columns (one for each) - `arr.max() = 4`, so we do `4 + 1` to get our required value of `5`.\n",
        "\n",
        "Now we use the current values in our `arr` of *\\[0, 1, 2, 3, 4\\]* to place `1` values in the correct positions of our presently zeros-only array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mveF8XhgdTdd",
        "outputId": "01faf133-8722-4e33-baf1-68c235cb82f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels[np.arange(num_samples), arr] = 1\n",
        "\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context:\n",
        "\n",
        "In your notebook, you're one-hot encoding the sentiment labels in the arr array.\n",
        "\n",
        "arr is initially a 1D NumPy array containing integers from 0 to 4, representing the sentiment classes.\n",
        "\n",
        "You then create a 2D array called labels, which is initialized with all zeros.\n",
        "\n",
        "The goal is to transform labels into a one-hot encoded representation of the sentiment labels.\n",
        "\n",
        "Explanation of labels[np.arange(num_samples), arr] = 1\n",
        "\n",
        "This line of code is a concise and efficient way to perform one-hot encoding using NumPy's advanced indexing capabilities. Let's break it down step by step:\n",
        "\n",
        "np.arange(num_samples):\n",
        "\n",
        "np.arange() is a NumPy function that creates a sequence of numbers.\n",
        "\n",
        "num_samples is the total number of samples in your dataset (which is the length of your DataFrame, len(df)).\n",
        "\n",
        "So, np.arange(num_samples) generates an array of integers from 0 to num_samples - 1. For example, if you have 10 samples, it will create [0, 1, 2, 3, 4, 5, 6, 7, 8, 9].\n",
        "\n",
        "This array will be used to index the rows of the labels array.\n",
        "arr:\n",
        "\n",
        "This is your original 1D NumPy array of sentiment labels (integers from 0 to 4).\n",
        "\n",
        "Each element in arr represents the sentiment class of the corresponding sample.\n",
        "\n",
        "This array will be used to index the columns of the labels array.\n",
        "labels[np.arange(num_samples), arr]:\n",
        "\n",
        "This is where the magic happens. This is called advanced indexing in NumPy, specifically using integer array indexing.\n",
        "\n",
        "You are using two arrays (np.arange(num_samples) and arr) to specify a set of coordinates (row, column) within the labels array.\n",
        "\n",
        "NumPy will pair up elements from these two arrays to form the coordinates. For example, if np.arange(num_samples) is [0, 1, 2] and arr is [2, 0, 1], then the coordinates would be (0, 2), (1, 0), and (2, 1).\n",
        "\n",
        "In effect, each element arr[i] specifies the column to access in row i.\n",
        "= 1:\n",
        "\n",
        "This assigns the value 1 to the elements of the labels array at the coordinates specified by the advanced indexing.\n",
        "In simpler terms:\n",
        "\n",
        "For each row (sample) in labels:\n",
        "\n",
        "The value in arr at that same index indicates which column should be set to 1.\n",
        "All other columns in that row will remain 0.\n",
        "Example:\n",
        "\n",
        "Let's say num_samples = 5, and arr = [2, 0, 4, 1, 3]. Then:\n",
        "\n",
        "np.arange(num_samples) will be [0, 1, 2, 3, 4].\n",
        "The indexing labels[np.arange(num_samples), arr] will result in the following coordinates:\n",
        "(0, 2) (row 0, column 2)\n",
        "(1, 0) (row 1, column 0)\n",
        "(2, 4) (row 2, column 4)\n",
        "(3, 1) (row 3, column 1)\n",
        "(4, 3) (row 4, column 3)\n",
        "The line labels[np.arange(num_samples), arr] = 1 will set the elements at those coordinates to 1.\n",
        "Explanation of labels\n",
        "\n",
        "After executing this line, the labels array will contain the one-hot encoded labels.\n",
        "\n",
        "Shape: The shape of labels is (num_samples, num_classes), where num_classes is the number of unique sentiment labels (in your case, 5: 0, 1, 2, 3, 4).\n",
        "Content: Each row in labels represents one sample, and it will have a single 1 at the index that corresponds to the sentiment label for that sample. All other elements in that row will be 0.\n",
        "Example labels array with the arr example above:\n",
        "\n",
        "\n",
        "[[0, 0, 1, 0, 0],  # Sample 0: Sentiment 2\n",
        " [1, 0, 0, 0, 0],  # Sample 1: Sentiment 0\n",
        " [0, 0, 0, 0, 1],  # Sample 2: Sentiment 4\n",
        " [0, 1, 0, 0, 0],  # Sample 3: Sentiment 1\n",
        " [0, 0, 0, 1, 0]]  # Sample 4: Sentiment 3\n",
        "Use code with caution\n",
        "In Essence\n",
        "\n",
        "The code labels[np.arange(num_samples), arr] = 1 is a very efficient way to map the integer class labels in arr to their one-hot encoded representation in labels.\n",
        "\n",
        "After this line labels will contain the one-hot encoded values."
      ],
      "metadata": {
        "id": "fFaC8NJqm46V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZ8RNfMdTdd"
      },
      "source": [
        "And there is our one-hot encoded labels array. Just like before, we save this to file as a Numpy binary file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhVm-snxdTdd"
      },
      "outputs": [],
      "source": [
        "with open('movie-labels.npy', 'wb') as f:\n",
        "    np.save(f, labels)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "playground",
      "language": "python",
      "name": "playground"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}