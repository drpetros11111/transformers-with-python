{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/transformers-with-python/blob/main/00_stopwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLtUXjpeTyJk"
      },
      "source": [
        "# Stopwords\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "TK add explanation of stopwords, + examples + some sample code + code we would actually use (eg NLTK)\n",
        "\n",
        "We will be using [this tweet](https://twitter.com/ivan_bezdomny/status/1367160747537682438) (don't worry, we will get to train some models):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIJ3sTorTyJn"
      },
      "outputs": [],
      "source": [
        "tweet = \"\"\"I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\n",
        "\n",
        "Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIUHVrDdTyJo"
      },
      "source": [
        "We will be using the **NLTK** library for removing stopwords. NLTK comes with several stopword corpora, we will be using the English corpus. This corpus contains a huge number of English stopwords like *a*, *the*, *be*, *for*, *do*, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Swr9D2wxTyJo",
        "outputId": "264d07be-1302-4e9f-aa92-f6abf22fb3c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "stop_words[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. from nltk.corpus import stopwords\n",
        "\n",
        "What it does: This line imports the stopwords module from the nltk.corpus (Natural Language Toolkit's corpus) package.\n",
        "\n",
        "Why it's used: NLTK is a powerful library for working with human language data (text).\n",
        "\n",
        "It contains many corpora (collections of text data) that are useful for natural language processing (NLP) tasks. One of these corpora is a collection of stopwords.\n",
        "\n",
        "In simple terms: This is like saying, \"I want to use the list of common words that NLTK provides.\"\n",
        "\n",
        "---------------------------\n",
        "#2. import nltk\n",
        "\n",
        "What it does: This line imports the main nltk library, which is necessary to download the resources.\n",
        "\n",
        "Why it's used: nltk.download('stopwords') is a method of this library and is used in the next line.\n",
        "\n",
        "In simple terms: This is like saying, \"I want to use the nltk library in the following code.\"\n",
        "\n",
        "----------------------\n",
        "#3.nltk.download('stopwords')\n",
        "\n",
        "##What it does:\n",
        "This line downloads the \"stopwords\" resource from NLTK's data repository.\n",
        "\n",
        "##Why it's used:\n",
        "The list of stopwords is not automatically included in the NLTK library itself.\n",
        "\n",
        "It's stored as a separate data file. This line ensures that you have the data needed to use stopwords.words('english') in the next step.\n",
        "\n",
        "You only need to run this once per Colab session or on a new machine.\n",
        "\n",
        "##In simple terms:\n",
        "\n",
        "This is like going to a library and getting the specific book you need (the \"stopwords\" list) before you can read it.\n",
        "\n",
        "--------------------------\n",
        "#4. stop_words = stopwords.words('english')\n",
        "\n",
        "##What it does:\n",
        "\n",
        "This is the core of the operation. It uses the stopwords.words() function to get a list of stopwords for the English language.\n",
        "\n",
        "The 'english' argument specifies that you want the English stopword list. The result (the list of stopwords) is then assigned to the variable stop_words.\n",
        "\n",
        "Why it's used: Stopwords (like \"the,\" \"a,\" \"is,\" \"in,\" etc.) are very common words that often don't add much meaning to text.\n",
        "\n",
        "In many NLP tasks (like text classification or topic modeling), it's helpful to remove these stopwords so that you can focus on the more important words.\n",
        "\n",
        "In simple terms: This line is saying, \"Give me the list of English stopwords, and I'll call that list 'stop_words' from now on.\"\n",
        "\n",
        "---------------------\n",
        "#5. print(stop_words[:5])\n",
        "\n",
        "##What it does:\n",
        "This line prints the first five elements of the stop_words list. [:5] is a Python slicing technique that extracts the elements from the beginning of the list up to (but not including) the element at index 5.\n",
        "\n",
        "##Why it's used:\n",
        "\n",
        "This is a way to quickly see a sample of what's in the stop_words list to confirm that the code is working and that you have the list you expect.\n",
        "\n",
        "##In simple terms:\n",
        "\n",
        "This line is saying, \"Show me the first five stopwords in the list so I can see what's there.\" To see the output, run the code.\n",
        "\n",
        "------------------------\n",
        "##Overall Goal\n",
        "\n",
        "The goal of this code is to obtain a list of common English stopwords that you can then use in subsequent text processing steps to remove these words from your text data.\n",
        "\n",
        "Why is this relevant to Google Colab?\n",
        "\n",
        "Data Science & NLP: Google Colab is a popular environment for data science and machine learning. Text processing is a very common task in these fields.\n",
        "\n",
        "##NLTK in Colab:\n",
        "\n",
        "NLTK is easy to install and use in Colab, making it a great choice for NLP work in this environment.\n",
        "\n",
        "##Text Preprocessing:\n",
        "\n",
        "Removing stopwords is a crucial part of text preprocessing, a common first step in many NLP pipelines.\n",
        "\n",
        "##Packages:\n",
        "\n",
        "Google Colab comes with many packages already installed, but sometimes it is necessary to download the data for the specific package as shown with nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "7tRe9-cIUSih"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRIVMYYoTyJp"
      },
      "source": [
        "Now we have a list of stopwords. When we process our text data we will iterate through each word, if it is present in `stop_words` it will be removed. To optimize the speed of the stopword lookup we can convert `stop_words` to a `set` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UtYSGpSTyJp"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop_words variable, which was originally a list, into a set.\n",
        "\n",
        "-------------------------------------\n",
        "Breakdown:\n",
        "\n",
        "##set():\n",
        "\n",
        "This is a built-in Python function that creates a set object.\n",
        "\n",
        "stop_words (inside the set()): This refers to the existing list of stopwords that we obtained from NLTK in the previous steps (e.g., ['i', 'me', 'my', 'myself', 'we', ...]).\n",
        "\n",
        "stop_words = ...: This assignment operator takes the newly created set and assigns it back to the variable stop_words, overwriting the original list.\n",
        "\n",
        "\n",
        "Why it's used (the reasoning):\n",
        "\n",
        "The key reason for converting the list to a set is to optimize the speed of stopword lookup. Here's why:\n",
        "\n",
        "##Lists vs. Sets:\n",
        "\n",
        "##Lists:\n",
        "\n",
        "Lists are ordered collections of items. When you check if an item is in a list (e.g., if word in my_list), Python has to potentially go through each item in the list one by one until it finds the word or reaches the end. This is called a linear search, and it can be slow, especially for long lists.\n",
        "\n",
        "##Sets:\n",
        "Sets are unordered collections of unique items. They are implemented using a data structure called a hash table, which allows for very fast membership testing.\n",
        "\n",
        "When you check if an item is in a set (e.g., if word in my_set), Python can determine if the item is present almost instantly, regardless of the set's size.\n",
        "\n",
        "This is called constant-time lookup.\n",
        "\n",
        "----------------------------\n",
        "##Stopword Removal:\n",
        "\n",
        "In NLP, you often need to check if a large number of words are present in your stopword list.\n",
        "\n",
        "If the stopword list is a list, each check would be slow. But, if the stopword list is a set, each check is very fast.\n",
        "\n",
        "##Efficiency:\n",
        "When you're dealing with large amounts of text data, the efficiency difference between using a list and a set for stopword lookup can be significant.\n",
        "\n",
        "Converting to a set can dramatically speed up your code.\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "Imagine you have a phone book.\n",
        "\n",
        "List: If the phone book was like a list, to find a number, you'd have to start at the first page and flip through each page one by one until you found the name you were looking for.\n",
        "\n",
        "Set: If the phone book was like a set, it would be indexed in a special way. You could open it up to roughly the right section and find the name almost immediately.\n",
        "\n",
        "Converting the stop_words list to a set is like indexing your phone book to make lookups much faster.\n",
        "\n",
        "In the context of Google Colab:\n",
        "\n",
        "Google Colab is often used for data science and machine learning, which frequently involves processing large amounts of text data.\n",
        "\n",
        "This makes the efficiency boost from using sets for stopword removal especially relevant.\n",
        "\n",
        "-----------------------------------------\n",
        "#Example:\n",
        "\n",
        "Let's say you have a very simple case:\n",
        "\n",
        "\n",
        "my_list = [\"apple\", \"banana\", \"orange\"]\n",
        "my_set = set(my_list)\n",
        "\n",
        "# Checking if \"banana\" is present:\n",
        "    print(\"banana\" in my_list)  # Can be slower\n",
        "    print(\"banana\" in my_set)   # Much faster\n",
        "\n",
        "While the difference is negligible for such small examples, imagine this with thousands of words in the list, and you are looking up thousands of words, and you have a better understanding of why sets are preferred for this specific task.\n",
        "\n",
        "In summary, stop_words = set(stop_words) is a crucial optimization step that significantly speeds up the process of removing stopwords from text data by leveraging the efficient lookup capabilities of Python sets."
      ],
      "metadata": {
        "id": "7aVd6pY5Wmuu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIN8BxMVTyJq"
      },
      "source": [
        "First we need to lowercase our text (because all of our stopwords are lowercased). Then we use split our input text into a list of tokens (each token is a word seperated by a space)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLboWzYDTyJq",
        "outputId": "674f3c1c-7de7-4dc9-a92d-5dc69df4d2c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i’m',\n",
              " 'amazed',\n",
              " 'how',\n",
              " 'often',\n",
              " 'in',\n",
              " 'practice,',\n",
              " 'not',\n",
              " 'only',\n",
              " 'does',\n",
              " 'a',\n",
              " '@huggingface',\n",
              " 'nlp',\n",
              " 'model',\n",
              " 'solve',\n",
              " 'your',\n",
              " 'problem,',\n",
              " 'but',\n",
              " 'one',\n",
              " 'of',\n",
              " 'their',\n",
              " 'public',\n",
              " 'finetuned',\n",
              " 'checkpoints,',\n",
              " 'is',\n",
              " 'good',\n",
              " 'enough',\n",
              " 'for',\n",
              " 'the',\n",
              " 'job.',\n",
              " 'both',\n",
              " 'impressed,',\n",
              " 'and',\n",
              " 'a',\n",
              " 'little',\n",
              " 'disappointed',\n",
              " 'how',\n",
              " 'rarely',\n",
              " 'i',\n",
              " 'get',\n",
              " 'to',\n",
              " 'actually',\n",
              " 'train',\n",
              " 'a',\n",
              " 'model',\n",
              " 'that',\n",
              " 'matters',\n",
              " ':(']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweet = tweet.lower().split()\n",
        "\n",
        "tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJkzfQ_dTyJr"
      },
      "source": [
        "And now we can iterate through the list, we check if each word exists in `stop_words` - if it does we discard it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alvzNggZTyJr",
        "outputId": "90e0fddf-fa05-468a-df28-9620498b0535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With stopwords: i’m amazed how often in practice, not only does a @huggingface nlp model solve your problem, but one of their public finetuned checkpoints, is good enough for the job. both impressed, and a little disappointed how rarely i get to actually train a model that matters :(\n",
            "Without: i’m amazed often practice, @huggingface nlp model solve problem, one public finetuned checkpoints, good enough job. impressed, little disappointed rarely get actually train model matters :(\n"
          ]
        }
      ],
      "source": [
        "tweet_no_stopwords = [word for word in tweet if word not in stop_words]\n",
        "\n",
        "print(\"With stopwords:\", ' '.join(tweet))\n",
        "print(\"Without:\", ' '.join(tweet_no_stopwords))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcpAKfQnTyJr"
      },
      "source": [
        "It's that easy! We'll move onto more preprocessing methods in the following sections."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}